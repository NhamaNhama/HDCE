# HDCE (Hybrid Dynamic Contextualization Embedding)

This project demonstrates a near-production-ready code structure for HDCE, leveraging:
- **Local Context Encoder** (Hugging Face Transformers + Context Gate)
- **Global Semantic Module** (PyTorch Geometric-based GNN)
- **Knowledge Interface** (Bloom Filter + Probabilistic Graph Walk)
- **Time-Series Memory** (KL-based detection + TimeDecay integration)
- **PyTorch Lightning** for training loops

## Quick Start

make install
make train

## Docker

make build-docker
docker run --rm -it hdce:latest

---

以下では、HDCE（Hybrid Dynamic Contextualization Embedding）の各構成要素がどのように連携し、そこに存在する数理的背景や実装上の工夫がどのように成り立っているのかを段階的に解説します。指摘されている複数の要素（動的3層構造、確率的意味マッピング、時系列適応機構、訓練プロトコルと評価基準、技術的課題など）がどのように相互作用し合っているかを、可能なかぎり詳細に説明していきます。なお、ここでは内部の検討過程を要約した形で示し、最終的な理解に重点を置いています。

【全体の概要】  
HDCE（Hybrid Dynamic Contextualization Embedding）は、大きく3つのレイヤ構造と、外部知識連携、確率的表現、時系列適応メカニズムが組み合わさって機能します。大まかに言えば、以下のような流れになります。  

1) 局所文脈エンコーダ: 個々のフレーズや文における文脈的特徴を精密に抽出する。  
   - 改良型の「Context Gate Attention」を導入し、単語間の矛盾や衝突度合いをゲートマスクで調整する。  
   - 埋め込みを量子化（8bit）しながら、超球面上の再マッピングで情報をなるべく失わない工夫を施す。  

2) 大域意味構造モジュール: テキスト全体に内在する大域的な論理関係や意味の骨格を抽象化して捉える。  
   - 3次元の意味グラフ構造を取り入れ、ノード間の因果・対比・包含などの関係を学習する。  
   - グラフ畳み込みを「関係型重み」や「経路長正規化」と組み合わせることで、柔軟かつ深い推論を実現。  

3) 外部知識インターフェイス: 内部で得た文脈表現や大域的関係を、リアルタイムに更新される知識グラフと突き合わせる。  
   - Bloom Filterを用いて高速に該当領域を絞り込み、PGS（確率的グラフ走査アルゴリズム）により深層関係を探索。  
   - 矛盾度などをベイジアン推定で統合し、最終的に信頼度の高い解釈を得る。  

これら3層に加えて、確率的表現の数理（確率超立方体空間や動的再重み付け層の導入）と、**時系列適応（神経履歴メモリやオンライン学習によるパーソナライズ）**が総合的に作用します。その結果、「テキストを静的に埋め込む」のではなく、「過去の文脈や外部知識、時間経過による情報更新を踏まえた動的な意味表現」を獲得するわけです。  

---

## 1. 動的3層構造の深層メカニズム

### 1.1 局所文脈エンコーダ（Local Context Encoder）

(1) 改良型Attention「Context Gate Attention」  
HDCEでは、通常のAttention機構に加えて、計算されたスコア行列にゲートマスク M_gate を要素ごとに乗算する設計を採用しています。矛盾度が高いペアほどAttentionの重みが抑制される仕組みになっており、単語同士の矛盾や衝突を軽減します。

(2) 量子化埋め込み圧縮  
768次元など大きなベクトルをそのまま扱うとメモリコストが高いため、8bit量子化と超球面上への再マッピングを組み合わせることで情報損失を最小化します。

### 1.2 大域意味構造モジュール（Global Semantic Module）

(1) 文書を3次元意味グラフとして構築  
ノードを主要概念、エッジを因果・対比などの関係として表し、さらに抽象度レベルの軸を持たせることで、大域的な論理構造を学習します。

(2) グラフ畳み込みの改良アルゴリズム  
Edgeごとに関係型重みW_rel(r)を用いるほか、パスの長さを利用した正規化を導入し、距離の長いノードからは寄与を小さくする工夫を取り入れています。

### 1.3 外部知識インターフェイス（Knowledge Interface）

(1) 知識グラフのリアルタイム検索  
Bloom Filter による事前絞り込みと、PGS（確率的グラフ走査）によるサンプリング探索を組み合わせ、外部知識と文章との不整合や補強情報を動的に取得します。

(2) 矛盾解決メカニズム  
外部知識との衝突が生じた際にはベイジアン推定を用いてテキスト・KBどちらの信頼度を優先すべきかを推定し、文脈表示を調整します。

---

## 2. 確率的意味マッピングの数理構造

### 2.1 確率超立方体表現

(1) 従来ベクトル空間を確率測度空間に拡張  
単語や概念を確率ベクトルとして扱うことで、多義性・曖昧性に対応できる柔軟な埋め込みを実現しています。

(2) 動的再重み付け層  
softmaxベースの再重み付けを特徴量（AmbiguityやTF-IDF等）と組み合わせ、文脈変化にともなう確率分布の更新を行います。

### 2.2 メタ意味オントロジー
WordNetを拡張した「WordNet++」などを用いて、生成され続ける新しい概念を三階述語論理で表現しながら外部知識に適応できる仕組みを備えています。

---

## 3. 時系列適応機構の詳細

### 3.1 神経履歴メモリ（Neural History Memory）
(1) 双方向LSTMとNeuromodulationの融合  
時系列上の文脈を双方向RNNで管理しつつ、TimeDecay等の関数で過去の状態を重み付けし、直近の情報を強調する仕組みを実装します。

(2) 重要イベント検出アルゴリズム  
KL発散によって、埋め込み分布が急激に変化したタイミングを検出し、大きな話題転換や重要度の高い情報が出現した際にメモリを更新します。

### 3.2 ユーザ適応型パラメータ調整
(1) オンライン学習による個人化  
ハイパーネットワークを併用し、ユーザ固有の言語使用傾向を学習することで、個人単位にパラメータを動的生成できます。

---

## 4. 訓練プロトコルの革新点

### 4.1 文脈矛盾検出タスク  
矛盾を検出するタスクを大規模に実施して事前学習・中間タスク学習を行い、文脈矛盾度を判断する能力を高めています。

### 4.2 推論経路予測  
知識グラフ内に埋め込まれた論理チェーンを逆方向にたどる学習を加え、中間的な論理ステップもモデルが補完しやすくなります。

---

## 5. 評価基準CASEの詳細

### 5.1 多層的文脈理解度（階層的Clozeテスト）  
表層単語レベルから抽象概念レベルまで、段階的に難易度を上げたClozeテストを使い、HDCEがどの程度深い文脈・概念を把握できるかを評価します。

### 5.2 時間的整合性 （会話軌跡再現テスト）  
対話や長文解析で、時間経過による話題変化や記憶の一貫性を検証。連続する質問への回答に矛盾がないかをチェックします。

---

## 6. 技術的課題と解決策

- リアルタイム知識統合の遅延: 「HotSpot Cache」や参照パターンの再利用で高速化。  
- 確率分布の次元爆発: スパース正則化や低ランク射影を組み合わせて不要なクラスタを削減。  
- 個人化パラメータの衝突: ハミング距離制約などで制御し、ユーザ間のパラメータ干渉を低減。

---

## 7. まとめと今後の展望

HDCE は静的な単語埋め込みを超えて「動的・総合的な認知プロセス」として言語理解を捉えるアーキテクチャです。  
- 局所文脈 + 大域構造 + 外部知識で多角的に意味を捉える。  
- 時系列メモリやパーソナライズでユーザ・時間変化へ適応する。  
- 大規模メモリや分散処理基盤を組み合わせることで、実務レベルにも対応できる可能性が高い。

従来の「単語を静的ベクトルに写像する」手法が、「言葉の写真を一瞬切り取る」ようなものであったとすれば、HDCE は「言葉が思考され、知識と連携し、時間の中で歩みを進めるプロセス」そのものを捉えようとしています。その点における革新性と発展可能性は非常に大きいと言えます。

